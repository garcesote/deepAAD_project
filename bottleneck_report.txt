`bottleneck` is a tool that can be used as an initial step for debugging
bottlenecks in your program.

It summarizes runs of your script with the Python profiler and PyTorch's
autograd profiler. Because your script will be profiled, please ensure that it
exits in a finite amount of time.

For more complicated uses of the profilers, please see
https://docs.python.org/3/library/profile.html and
https://pytorch.org/docs/main/autograd.html#profiler for more information.
Running environment analysis...
Running your script with cProfile
Training Conformer on all subjects with fulsang data...
Using criterion: mean | CV fold: None
Number of parameters: 0.10M
Model size: 0.10M
Epoch: 0 | Train loss: -0.0482 | Val loss/acc: -0.1038/57.0582
Epoch: 1 | Train loss: -0.0929 | Val loss/acc: -0.1188/57.5398
Epoch: 2 | Train loss: -0.1074 | Val loss/acc: -0.1284/58.4661
Epoch: 3 | Train loss: -0.1155 | Val loss/acc: -0.1332/59.4665
Epoch: 4 | Train loss: -0.1198 | Val loss/acc: -0.1367/59.4294
Epoch: 5 | Train loss: -0.1232 | Val loss/acc: -0.1391/59.8370
Epoch: 6 | Train loss: -0.1262 | Val loss/acc: -0.1419/60.2075
Epoch: 7 | Train loss: -0.1288 | Val loss/acc: -0.1438/60.3557
Epoch: 8 | Train loss: -0.1310 | Val loss/acc: -0.1444/60.3186
Epoch: 9 | Train loss: -0.1325 | Val loss/acc: -0.1454/60.5780
Epoch: 10 | Train loss: -0.1340 | Val loss/acc: -0.1455/60.4668
Epoch: 11 | Train loss: -0.1361 | Val loss/acc: -0.1444/60.3557
Epoch: 12 | Train loss: -0.1373 | Val loss/acc: -0.1455/60.6150
Epoch: 13 | Train loss: -0.1391 | Val loss/acc: -0.1470/60.5409
Epoch: 14 | Train loss: -0.1405 | Val loss/acc: -0.1446/60.3927
Epoch: 15 | Train loss: -0.1420 | Val loss/acc: -0.1458/60.5039
Epoch: 16 | Train loss: -0.1433 | Val loss/acc: -0.1465/60.7262
Epoch: 17 | Train loss: -0.1447 | Val loss/acc: -0.1467/61.3190
Epoch: 18 | Train loss: -0.1454 | Val loss/acc: -0.1447/60.9114
Running your script with the autograd profiler...
Training Conformer on all subjects with fulsang data...
Using criterion: mean | CV fold: None
Number of parameters: 0.10M
Model size: 0.10M
